


%pwd


cd ..








import numpy as np
import os
import pandas as pd
import re
from sklearn.ensemble import RandomForestClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold





all_df = pd.read_csv("Data/train.csv")
all_df.head(20)


all_df['Survived'].value_counts()


all_df.shape


all_df.columns





all_df = all_df[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp',
       'Parch', 'Fare', 'Cabin', 'Embarked']]


all_df.head()


all_df['Cabin_Count'] = all_df['Cabin'].str.count(' ') + 1
all_df['Cabin_level'] = all_df['Cabin'].str[0]


def extract_first_number(cabin_string):
    if pd.isna(cabin_string):
        return None  # Return None if the cabin string is NaN
    numbers = re.findall(r'\d+', cabin_string)
    return int(numbers[0]) if numbers else None

all_df['First_Number'] = all_df['Cabin'].apply(extract_first_number)


all_df['Cabin_level'].value_counts()


all_df['Sex'] = all_df['Sex'].replace({'male': 0, 'female': 1})
cabin_mapping = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'T': 8}
all_df['Cabin_level'] = all_df['Cabin_level'].map(cabin_mapping)


Embarked_mapping = {'S': 1, 'C': 2, 'Q': 3}
all_df['Embarked'] = all_df['Embarked'].map(Embarked_mapping)


all_df.head()


all_df.columns


all_df = all_df[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp',
       'Parch', 'Fare', 'Embarked', 'Cabin_Count', 'Cabin_level', 'First_Number']]


all_df = all_df.fillna(0) 


all_df.head()


X, y = all_df.drop(columns = ['Survived']), all_df['Survived']








X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=326)


rf_classifier = RandomForestClassifier(n_estimators=100, random_state=326)  # n_estimators is the number of trees
rf_classifier.fit(X_train, y_train)





y_pred = rf_classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)














param_grid = {
    'n_estimators': [25, 50, 75, 100],  # Number of trees in the forest
    'max_features': [6, 7, 8, 9, 10],  # Number of features to consider at every split
    'max_depth': [10, 12, 14, 16, 18],  # Maximum number of levels in tree
    'criterion': ['gini', 'entropy'],  # Function to measure the quality of a split
    'min_samples_split':[2, 4, 6, 8, 10],
    'max_samples': [0.7, 0.8, 0.9]
    
}




rf_classifier = RandomForestClassifier(random_state=42)
stratified_kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=326)
grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=stratified_kfold, 
                           scoring='accuracy', verbose=1, n_jobs=-1)



grid_search.fit(X, y)


print("Best parameters:", grid_search.best_params_)
print("Best cross-validation score (best accuracy):", grid_search.best_score_)


pd.DataFrame(grid_search.cv_results_).sort_values(by='mean_test_score', ascending=False).head(10)


results = pd.DataFrame(grid_search.cv_results_)
top_models = results.nlargest(10, 'mean_test_score')


base_models = []
for index, row in top_models.iterrows():
    model = RandomForestClassifier(n_estimators=row['param_n_estimators'],
                                   max_features=row['param_max_features'],
                                   max_depth=row['param_max_depth'],
                                   criterion=row['param_criterion'],
                                   min_samples_split=row['param_min_samples_split'],
                                   max_samples=row['param_max_samples'],
                                   random_state=index)
    base_models.append(('rf_{}'.format(index), model))



# Logistic Regression as the final estimator
final_estimator = LogisticRegression()

# Stacking Classifier
stacking_classifier = StackingClassifier(estimators=base_models, final_estimator=final_estimator, cv=10)


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
stacking_classifier.fit(X_train, y_train)


# Predictions
y_pred = stacking_classifier.predict(X_test)

# Performance
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, y_pred)
print("Stacking model accuracy:", accuracy)









test_df = all_df = pd.read_csv("/kaggle/input/titanic/test.csv")


Results = test_df[['PassengerId']]


test_df = test_df[[ 'Pclass', 'Sex', 'Age', 'SibSp',
       'Parch', 'Fare', 'Cabin', 'Embarked']]


test_df.head()


test_df['Cabin_Count'] = test_df['Cabin'].str.count(' ') + 1
test_df['Cabin_level'] = test_df['Cabin'].str[0]


def extract_first_number(cabin_string):
    if pd.isna(cabin_string):
        return None  # Return None if the cabin string is NaN
    numbers = re.findall(r'\d+', cabin_string)
    return int(numbers[0]) if numbers else None

test_df['First_Number'] = test_df['Cabin'].apply(extract_first_number)


test_df['Sex'] = test_df['Sex'].replace({'male': 0, 'female': 1})
cabin_mapping = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'T': 8}
test_df['Cabin_level'] = test_df['Cabin_level'].map(cabin_mapping)


Embarked_mapping = {'S': 1, 'C': 2, 'Q': 3}
test_df['Embarked'] = test_df['Embarked'].map(Embarked_mapping)


test_df.head()


test_df.columns


test_df = test_df[['Pclass', 'Sex', 'Age', 'SibSp',
       'Parch', 'Fare', 'Embarked', 'Cabin_Count', 'Cabin_level', 'First_Number']]


test_df = test_df.fillna(0) 


pred_y = stacking_classifier.predict(test_df)


Results['Survived'] = pred_y


Results


Results.to_csv('submission.csv', index=False)









